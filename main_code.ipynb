{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d3d418-e712-4ee2-a583-6de91f346c28",
   "metadata": {},
   "source": [
    "# Analysis Grand Challenge of the PHYSLITE data format\n",
    "\n",
    "This notebook is written as the result of a project of the IRIS-HEP summer fellowship programme. The web page, dedicated to the project, can be found via the [link](https://iris-hep.org/fellows/Denys-Klekots.html).\n",
    "\n",
    "This code is heavily based on the Analysis Grand Challenge (AGC) of the CMS Open Data $t\\bar{t}$. There is a link to the corresponding [GitHub repository](https://github.com/alexander-held/PyHEP-2022-AGC) written by Alexander Held.\n",
    "\n",
    "Here we will use scientific Python infrastructure, which allows us to read data from the file and further analyse it. The data in high-energy physics is usually stored on an event-by-event basis and contains information on the detected particles. As there is an arbitrary number of particles for each event, we will use the `awkward` Python tools which allow us to handle arrays with an arbitrary length. The PHYSLITE format is stored inside a widely used in higher energy physics `*.root` file format and can be read by using the `coffee` python package. Also, the different events in high-energy physics are independent of each other, which allows for parallel computation and distribution over the cluster, the `dask` python package allows one to handle parallel computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e1eb49c-2d2f-45a3-924f-1d7da14131ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import dask\n",
    "import cabinetry\n",
    "import dask_awkward as dak\n",
    "import numpy as np\n",
    "import hist.dask\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from coffea.nanoevents import NanoEventsFactory, PHYSLITESchema\n",
    "from coffea import dataset_tools\n",
    "\n",
    "\n",
    "from dask.distributed import Client, performance_report\n",
    "\n",
    "# create a folder for output tracking of uproot.open setup\n",
    "MEASUREMENT_PATH = Path(datetime.datetime.now().strftime(\"measurements/%Y-%m-%d_%H-%M-%S\"))\n",
    "os.makedirs(MEASUREMENT_PATH)\n",
    "\n",
    "# Set up the client distribution calculation for coffea-opendata.casa. Change the URL for UChicago\n",
    "client = Client(\"tls://localhost:8786\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216b7211-6c29-4a2d-a1d6-1c5c810f98aa",
   "metadata": {
    "tags": []
   },
   "source": [
    "The PHYSLITE files used for this analysis contain a lot of information, organised in brunches, a description of which can be accessed via the [link](https://atlas-physlite-content-opendata.web.cern.ch/). For the purpose of this specific analysis, a few of them are used and a filter is applied to select used ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44d6e5fe-af3c-4cb2-9567-70d51b1c206a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_name(name):\n",
    "    return name in [\n",
    "        \"AnalysisElectronsAuxDyn.pt\",\n",
    "        \"AnalysisElectronsAuxDyn.eta\",\n",
    "        \"AnalysisElectronsAuxDyn.phi\",\n",
    "        \"AnalysisElectronsAuxDyn.m\",\n",
    "        \n",
    "        \"AnalysisMuonsAuxDyn.pt\",\n",
    "        \"AnalysisMuonsAuxDyn.eta\",\n",
    "        \"AnalysisMuonsAuxDyn.phi\",\n",
    "        \"AnalysisMuonsAuxDyn.m\",\n",
    "        \n",
    "        \"AnalysisJetsAuxDyn.pt\",\n",
    "        \"AnalysisJetsAuxDyn.eta\",\n",
    "        \"AnalysisJetsAuxDyn.phi\",\n",
    "        \"AnalysisJetsAuxDyn.m\",\n",
    "        \n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb\",\n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pc\",\n",
    "        \"BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu\",\n",
    "        \n",
    "        \"EventInfoAuxDyn.mcEventWeights\",\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71381b99-ac0e-4d0c-bfe2-b6387c8b08cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "The code in the cell below will read the data from the file in a function NanoEventsFactory.from_root and produce an awkward array which is named events in this code. Here the PHYSLITESchema is specified to correspond to the format of the data and filter which was discussed above. Also the delayed computation is specified, which make computation in a single go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1525911-4af1-4721-ac2a-56bd0ad4f526",
   "metadata": {},
   "source": [
    "The file used in the next cell is one of the Montecarlo-generated files for the $t \\bar{t}$ pair physical processed. See the code below for more information and a reference to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a13a46d3-249b-459b-92f5-fb708c59160e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mc_file = \"root://xcache.af.uchicago.edu:1094//root://eospublic.cern.ch//eos/opendata/atlas/rucio/mc20_13TeV/DAOD_PHYSLITE.37620644._000012.pool.root.1\"\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    {mc_file: \"CollectionTree\"},\n",
    "    schemaclass=PHYSLITESchema,\n",
    "    delayed=True,\n",
    "    uproot_options=dict(filter_name=filter_name),\n",
    ").events()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5744c0-b9f7-4bf8-8e85-95bcc99c80b0",
   "metadata": {},
   "source": [
    "### B-tagging discriminant\n",
    "\n",
    "The DL1 b-tagging flavour tagging algorithm is used for pre-analysis of data stored in PHYSLITE format and used in this analysis. The output of the DL1 algorithm is the $p_b$ $p_c$ and $p_u$ variables that are combined by the following formula to define the final discriminant on b-tagging\n",
    "\n",
    "$$\n",
    "D_{DL1} = log \\left( \\frac{p_b}{f_c \\cdot p_c + (1-f_c) \\cdot p_u} \\right)\n",
    "$$ \n",
    "\n",
    "The $p_b$ $p_c$ and $p_u$ are stored in the file under field names `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb`, `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pb` and `BTagging_AntiKt4EMPFlowAuxDyn.DL1dv01_pu` respectively. For more information about discriminant value please refer to the following [link](https://ftag.docs.cern.ch/recommendations/algs/2019-recommendations/#algorithm-structure) (Please note that the CERN account might be needed). $f_c$ is the constant which equal to $f_c = 0.018$ in this analysis.\n",
    "\n",
    "The jet is considered as b-tagged if the $D_{DL1}$ variable is above threshold. The threshold value of 2.456 was used here, which correspond to efficiency of 77%. Refer to the [link](https://ftag.docs.cern.ch/recommendations/algs/r22-preliminary/#working-point-definition-for-dl1dv01) for more information (please note that CERN account might be needed to access)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0430fa53-191b-49e1-8c94-c657b4ebcf68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_jets_btagDL1d(events):\n",
    "    \n",
    "    BTagging = events.BTagging_AntiKt4EMPFlow\n",
    "    \n",
    "    f_c = 0.018\n",
    "    DDL1 = BTagging.DL1dv01_pb/(f_c*BTagging.DL1dv01_pc + (1-f_c)*BTagging.DL1dv01_pu)\n",
    "    DDL1 = np.log(DDL1)\n",
    "\n",
    "    return DDL1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18310875-60ae-4d0a-b73e-fff01c4dd882",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Events selection\n",
    "\n",
    "The point of this analysis grand challenge is to make analysis of the t quark pairs. Specifically here is selected events which has at least four quark jets, at least two of tham is b-tagged, and exactly one charged lepton. The additional selection on the transverse momentum and the $\\eta$ parameter of the leptons and jets are applied.\n",
    "\n",
    "The schematic view of event with four jets is presented on the following image. Here who b quarks are forming jets that is b-tagged. The who another quarks (which coming from W boson decay) is not necessary b-tagged. The charged lepton is also coming from decay of other W boson, and the neutrino is not detected in this experiment.\n",
    "\n",
    "<div>\n",
    "<img src=\"utils/ttbar.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "The source of the image is the [AGC](https://github.com/alexander-held/PyHEP-2022-AGC/blob/main/talk.ipynb) notebook by Alexander Held.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c1584-d382-4117-a4bb-824c1a84816b",
   "metadata": {},
   "source": [
    "### Notes on b-tagging variables in code\n",
    "\n",
    "At the moment of writing of this code, the jets information are stored in the file under `AnalysisJetsAuxDyn` brunch name, at the same time the variables, used for calculation of the b-tagging discriminant, are stored in the brunch named `BTagging_AntiKt4EMPFlowAuxDyn`. In the future it might be changes in the `PHYSLITESchema` to link the b-tagging infro into jet's branch, but as for now we did in manually. The usage of the b-tagging discriminant under the jets brunch is much more conveniant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f7215-98a3-4588-8c29-8393f1883038",
   "metadata": {},
   "source": [
    "### Trijet mass\n",
    "\n",
    "In the function below all the jets (which pass the present) in each event are combined in all possible groups of three, after which the group with the largest b-tagging value is selected and the invariant mass of which is calculated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c128714-39e9-4bb1-bc34-e3168c6232da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_trijet_mass_and_ev_filter(events):\n",
    "    # pT > 30 GeV for leptons, > 25 GeV for jets\n",
    "    selected_electrons = events.Electrons[events.Electrons.pt > 30 & (np.abs(events.Electrons.eta) < 2.1)]\n",
    "    selected_muons = events.Muons[events.Muons.pt > 30 & (np.abs(events.Muons.eta) < 2.1)]\n",
    "    \n",
    "    # calculate tagging variable and attach it to jets(which pass the present)\n",
    "    jets = events.Jets\n",
    "    jets[\"btagDL1d\"] = calculate_jets_btagDL1d(events)\n",
    "    selected_jets = jets[events.Jets.pt > 25 & (np.abs(events.Jets.eta) < 2.4)] \n",
    "    \n",
    "    # single lepton requirement\n",
    "    event_filters = ((ak.count(selected_electrons.pt, axis=1) + ak.count(selected_muons.pt, axis=1)) == 1)\n",
    "    # at least four jets\n",
    "    event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 4)\n",
    "    # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "    B_TAG_THRESHOLD = 2.456\n",
    "    event_filters = event_filters & (ak.sum(selected_jets.btagDL1d > B_TAG_THRESHOLD, axis=1) >= 2)\n",
    "    \n",
    "    # apply filters\n",
    "    selected_jets = selected_jets[event_filters]\n",
    "    \n",
    "    trijet = ak.combinations(selected_jets, 3, fields=[\"j1\", \"j2\", \"j3\"])  # trijet candidate\n",
    "    trijet[\"p4\"] = trijet.j1 + trijet.j2 + trijet.j3  # four-momentum of tri-jet system\n",
    "\n",
    "    trijet[\"max_btag\"] = np.maximum(trijet.j1.btagDL1d, np.maximum(trijet.j2.btagDL1d, trijet.j3.btagDL1d))\n",
    "    trijet = trijet[trijet.max_btag > B_TAG_THRESHOLD]  # at least one-btag in trijet candidates\n",
    "        \n",
    "    # pick trijet candidate with largest pT and calculate mass of system\n",
    "    trijet_mass = trijet[\"p4\"][ak.argmax(trijet.p4.pt, axis=1, keepdims=True)].mass\n",
    "    \n",
    "    \n",
    "    return ak.flatten(trijet_mass), event_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b0e840-0390-442c-8f7e-4080fbaf5b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reconstructed_top_mass, _ = calculate_trijet_mass_and_ev_filter(events)\n",
    "hist_reco_mtop = hist.dask.Hist.new.Reg(16, 0, 375, label=\"$m_{bjj} [GeV]$\").Double().fill(reconstructed_top_mass/1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae263be-6b12-4bb8-b731-fe9835aaba49",
   "metadata": {},
   "source": [
    "The code for processing the data written above is not executed until the `.compute()` method of the `hist_reco_mtop` is not called. This was discussed above when one mentioned delayed computation in the parameters of `NanoEventsFactory.from_root()` function. Such a delayed approach allows highly improved efficiency of calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85939ab4-df2f-43a7-9876-a8c681377ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # perform computation and visualize\n",
    "# artists = hist_reco_mtop.compute().plot()\n",
    "\n",
    "# # and annotate the visualization\n",
    "# fig_dir = Path.cwd() / \"figures\"\n",
    "# fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ax = artists[0].stairs.axes\n",
    "# fig = ax.get_figure()\n",
    "# ax.vlines(175, 0, ax.get_ylim()[1], colors=[\"grey\"], linestyle=\"dotted\") \n",
    "# ax.text(0.5, 0.1, \"$m_{t} = 175$ GeV\", transform = ax.transAxes)\n",
    "# ax.set_xlim([0, 375])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44805789-7fb2-445a-ae52-b126e39b5193",
   "metadata": {},
   "source": [
    "# Distributed computations\n",
    "\n",
    "Now we will move to the distributed computations. The code below applies the same approach to the selection of the data and calculation of the mass of the trijets. The further code uses multiple PHYSLITE files which have the same structure of branches and are processed in a parallel way, after which the results are merged in the final histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4741a-e1e7-4df2-9c86-5ee5d1cdb1c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "The files used for distributed calculations and their metadata are specified in the cell below. This particular case uses Monte Carlo generated data which can be accessed via the [link](https://opendata.cern.ch/record/80017). The corresponding metadata can be accessed via the [link](https://opendata.atlas.cern/docs/documentation/overview_data/data_research_2024/#metadata). Please refer to the comments in the file_utils.py file for more information. \n",
    "\n",
    "In this example, the files from the CERN server are cached wiht `xcache`, which allows for significant speed up data retrieval if the data is used repeatedly. The prefix to the data URL can be managed with the `XCACHE_PREFIX` variable in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fadf1-e58a-4b72-8c8c-b2cbe495edc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "The metadata is read from the metadata.csv file during the importing of `file_utils`. All of the available metadata for specific processes is stored, but only some of them are used (see below), this redundancy affects performance very little but is much more favourable from the code flexibility and user-friendliness point of view. \n",
    "\n",
    "Additionally, the label of physicall process is added to the metadata dictionary in the cell below, this label is used during the drawing of the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d67087-15aa-4429-8d5b-4e64a63168d2",
   "metadata": {},
   "source": [
    "## Monte Carlo-simulated physical processes of $t$ quarks producing.\n",
    "\n",
    "The processes of producing single top quarks and mater-antimatter $t \\bar{t}$ pairs were simulated in the input data for this analysis. The Feynman diagrams of the leading order for single top processes are described in the picture. The original of the picture is Ref. [1]\n",
    "\n",
    "<div>\n",
    "<img src=\"utils/Single_quark_process_diagrams.png\" width=\"1000\"/>\n",
    "</div>\n",
    "The diagrams shows single top production: (a) - t-channel, (b) - Wt -channel, (c) s-channel.\n",
    "\n",
    "\n",
    "[1] Precision Measurements of Top Quark Production with the ATLAS Detector. Philipp  Stolte. EPJ Web Conf. 137 08015 (2017). [DOI:10.1051/epjconf/201713708015](https://doi.org/10.1051/epjconf/201713708015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9dff7f0-51a3-46b0-92a0-049a55f411bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Physics: PowhegPythia8EvtGen_A14_singletop_schan_lept_top       | Description: POWHEG+Pythia8+EvtGen single-top-quark s-channel production (top), inclusive, A14 tune, ME NNPDF30 NLO, A14 NNPDF23 LO'\n",
      "Physics: PowhegPythia8EvtGen_A14_singletop_schan_lept_antitop   | Description: POWHEG+Pythia8+EvtGen single-top-quark s-channel production (anti-top), inclusive, A14 tune, ME NNPDF30 NLO, A14 NNPDF23 LO'\n",
      "Physics: PhPy8EG_A14_tchan_BW50_lept_top                        | Description: POWHEG+Pythia8+EvtGen single-top-quark t-channel (2->3) production (top),MadSpin, A14 tune, ME NNPDF3.04f NLO, A14 NNPDF23 LO'\n",
      "Physics: PhPy8EG_A14_tchan_BW50_lept_antitop                    | Description: POWHEG+Pythia8+EvtGen single-top-quark t-channel (2->3) production (top),MadSpin, A14 tune, ME NNPDF3.04f NLO, A14 NNPDF23 LO'\n",
      "Physics: PhPy8EG_tW_dyn_DR_incl_antitop                         | Description: POWHEG+Pythia8+EvtGen tW production (antitop), DR scheme, dynamic scale, inclusive, hdamp equal 1.5*top mass, A14 tune, ME NNPDF30 NLO, A14 NNPDF23 LO'\n",
      "Physics: PhPy8EG_tW_dyn_DR_incl_top                             | Description: POWHEG+Pythia8+EvtGen tW production (antitop), DR scheme, dynamic scale, inclusive, hdamp equal 1.5*top mass, A14 tune, ME NNPDF30 NLO, A14 NNPDF23 LO'\n",
      "Physics: PhPy8EG_A14_ttbar_hdamp258p75_nonallhad                | Description: POWHEG+Pythia8 ttbar production with Powheg hdamp equal 1.5*top mass, A14 tune, at least one lepton filter, ME NNPDF30 NLO, A14 NNPDF23 LO from DSID 410450 LHE files with Shower Weights added '\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "File did not read properly: [ERROR] Operation expired",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_dict \u001b[38;5;129;01min\u001b[39;00m fileset\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhysics: \u001b[39m\u001b[38;5;132;01m{phys:55s}\u001b[39;00m\u001b[38;5;124m| Description: \u001b[39m\u001b[38;5;132;01m{desc:s}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(phys\u001b[38;5;241m=\u001b[39mfile_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphysics_short\u001b[39m\u001b[38;5;124m\"\u001b[39m], desc\u001b[38;5;241m=\u001b[39mfile_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m\"\u001b[39m] ))\n\u001b[0;32m---> 48\u001b[0m samples, _ \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_tools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfileset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/coffea/dataset_tools/preprocess.py:330\u001b[0m, in \u001b[0;36mpreprocess\u001b[0;34m(fileset, step_size, align_clusters, recalculate_steps, files_per_batch, skip_bad_files, file_exceptions, save_form, scheduler, uproot_options, step_size_safety_factor)\u001b[0m\n\u001b[1;32m    314\u001b[0m     dak_norm_files \u001b[38;5;241m=\u001b[39m dask_awkward\u001b[38;5;241m.\u001b[39mfrom_awkward(\n\u001b[1;32m    315\u001b[0m         ak_norm_files, math\u001b[38;5;241m.\u001b[39mceil(\u001b[38;5;28mlen\u001b[39m(ak_norm_files) \u001b[38;5;241m/\u001b[39m files_per_batch)\n\u001b[1;32m    316\u001b[0m     )\n\u001b[1;32m    317\u001b[0m     files_to_preprocess[name] \u001b[38;5;241m=\u001b[39m dask_awkward\u001b[38;5;241m.\u001b[39mmap_partitions(\n\u001b[1;32m    318\u001b[0m         get_steps,\n\u001b[1;32m    319\u001b[0m         dak_norm_files,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         uproot_options\u001b[38;5;241m=\u001b[39muproot_options,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[0;32m--> 330\u001b[0m (all_processed_files,) \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_to_preprocess\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, processed_files \u001b[38;5;129;01min\u001b[39;00m all_processed_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    333\u001b[0m     processed_files_without_forms \u001b[38;5;241m=\u001b[39m processed_files[\n\u001b[1;32m    334\u001b[0m         [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_path\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msteps\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_entries\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muuid\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    335\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask/base.py:661\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    658\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    660\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 661\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/dask_awkward/lib/core.py:1903\u001b[0m, in \u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1901\u001b[0m     len_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_args\n\u001b[1;32m   1902\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwarg_repacker(args_deps_expanded[len_args:])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 1903\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/coffea/dataset_tools/preprocess.py:74\u001b[0m, in \u001b[0;36mget_steps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 74\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m     76\u001b[0m tree \u001b[38;5;241m=\u001b[39m the_file[arg\u001b[38;5;241m.\u001b[39mobject_path]\n\u001b[1;32m     77\u001b[0m num_entries \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mnum_entries\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/coffea/dataset_tools/preprocess.py:68\u001b[0m, in \u001b[0;36mget_steps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m lz_or_nf:\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 68\u001b[0m         the_file \u001b[38;5;241m=\u001b[39m uproot\u001b[38;5;241m.\u001b[39mopen({arg\u001b[38;5;241m.\u001b[39mfile: \u001b[38;5;28;01mNone\u001b[39;00m}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39muproot_options)\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m file_exceptions \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     70\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m skip_bad_files:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/uproot/reading.py:142\u001b[0m, in \u001b[0;36mopen\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_path, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(file_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a string, pathlib.Path, an object with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseek\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m methods, or a length-1 dict of \u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;124mfile_path: object_path}, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m     )\n\u001b[0;32m--> 142\u001b[0m file \u001b[38;5;241m=\u001b[39m ReadOnlyFile(\n\u001b[1;32m    143\u001b[0m     file_path,\n\u001b[1;32m    144\u001b[0m     object_cache\u001b[38;5;241m=\u001b[39mobject_cache,\n\u001b[1;32m    145\u001b[0m     array_cache\u001b[38;5;241m=\u001b[39marray_cache,\n\u001b[1;32m    146\u001b[0m     custom_classes\u001b[38;5;241m=\u001b[39mcustom_classes,\n\u001b[1;32m    147\u001b[0m     decompression_executor\u001b[38;5;241m=\u001b[39mdecompression_executor,\n\u001b[1;32m    148\u001b[0m     interpretation_executor\u001b[38;5;241m=\u001b[39minterpretation_executor,\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions,\n\u001b[1;32m    150\u001b[0m )\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m object_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mroot_directory\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/uproot/reading.py:573\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m<\u001b[39m _file_header_fields_big\u001b[38;5;241m.\u001b[39msize:\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m is not enough to read the TFile header (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    568\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    569\u001b[0m             _file_header_fields_big\u001b[38;5;241m.\u001b[39msize,\n\u001b[1;32m    570\u001b[0m         )\n\u001b[1;32m    571\u001b[0m     )\n\u001b[0;32m--> 573\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_source\u001b[38;5;241m.\u001b[39mchunk(\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbegin_chunk_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    575\u001b[0m )\u001b[38;5;241m.\u001b[39mdetach_memmap()\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhook_before_interpret()\n\u001b[1;32m    579\u001b[0m (\n\u001b[1;32m    580\u001b[0m     magic,\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fVersion,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_begin_chunk, _file_header_fields_small, {}\n\u001b[1;32m    596\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/uproot/source/fsspec.py:115\u001b[0m, in \u001b[0;36mchunk\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh\u001b[38;5;241m.\u001b[39mseek(start)\n\u001b[0;32m--> 115\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fh\u001b[38;5;241m.\u001b[39mread(stop \u001b[38;5;241m-\u001b[39m start)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    117\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39mcat_file(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file_path, start, stop)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/fsspec/spec.py:1846\u001b[0m, in \u001b[0;36mread\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1844\u001b[0m     \u001b[38;5;66;03m# don't even bother calling fetch\u001b[39;00m\n\u001b[1;32m   1845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1846\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache\u001b[38;5;241m.\u001b[39m_fetch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m length)\n\u001b[1;32m   1847\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(out)\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/fsspec/caching.py:189\u001b[0m, in \u001b[0;36m_fetch\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m     part \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    188\u001b[0m end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, end \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocksize)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfetcher(start, end)  \u001b[38;5;66;03m# new block replaces old\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m=\u001b[39m start\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/fsspec_xrootd/xrootd.py:754\u001b[0m, in \u001b[0;36m_fetch_range\u001b[0;34m()\u001b[0m\n\u001b[1;32m    750\u001b[0m status, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_myFile\u001b[38;5;241m.\u001b[39mread(\n\u001b[1;32m    751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetaOffset \u001b[38;5;241m+\u001b[39m start, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetaOffset \u001b[38;5;241m+\u001b[39m end \u001b[38;5;241m-\u001b[39m start, timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\n\u001b[1;32m    752\u001b[0m )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m status\u001b[38;5;241m.\u001b[39mok:\n\u001b[0;32m--> 754\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile did not read properly: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstatus\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[0;31mOSError\u001b[0m: File did not read properly: [ERROR] Operation expired"
     ]
    }
   ],
   "source": [
    "import file_utils\n",
    "\n",
    "XCACHE_PREFIX = \"root://xcache.af.uchicago.edu:1094//\" # this could be an empty string\n",
    "\n",
    "fileset = {\n",
    "\"singletop_schan_lept_top\"  : \n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_singletop_schan_lept_top},\n",
    "            'metadata': {**file_utils.metadata_singletop_schan_lept_top, \"process-label\":\"single top s chan.\"},\n",
    "            },\n",
    "\"singletop_schan_lept_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_singletop_schan_lept_antitop},\n",
    "            'metadata': {**file_utils.metadata_singletop_schan_lept_antitop, \"process-label\":\"single top s chan.\"},\n",
    "            },\n",
    "\"tchan_BW50_lept_top\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_tchan_BW50_lept_top},\n",
    "            'metadata': {**file_utils.metadata_tchan_BW50_lept_top, \"process-label\":\"single top t chan.\"},\n",
    "            },\n",
    "\"tchan_BW50_lept_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_tchan_BW50_lept_antitop},\n",
    "            'metadata': {**file_utils.metadata_tchan_BW50_lept_antitop, \"process-label\":\"single top t chan.\"},\n",
    "            },\n",
    "\"tW_dyn_DR_incl_antitop\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_tW_dyn_DR_incl_antitop},\n",
    "            'metadata': {**file_utils.metadata_tW_dyn_DR_incl_antitop, \"process-label\":\"single top tW chan.\"},\n",
    "            },\n",
    "\"tW_dyn_DR_incl_top\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_tW_dyn_DR_incl_top},\n",
    "            'metadata': {**file_utils.metadata_tW_dyn_DR_incl_top, \"process-label\":\"single top tW chan.\"},\n",
    "            },\n",
    "\"ttbar_hdamp258p75_nonallhad\":\n",
    "            {\n",
    "            'files': {XCACHE_PREFIX+name:'CollectionTree' for name in file_utils.filenames_ttbar_hdamp258p75_nonallhad},\n",
    "            'metadata': {**file_utils.metadata_ttbar_hdamp258p75_nonallhad, \"process-label\":\"$t \\\\bar{t}$\"},\n",
    "            }\n",
    "}      \n",
    "    \n",
    "# Let's print the description from the metadata.\n",
    "for file_dict in fileset.values():\n",
    "    \n",
    "    print(\"Physics: {phys:55s}| Description: {desc:s}\".format(phys=file_dict[\"metadata\"][\"physics_short\"], desc=file_dict[\"metadata\"][\"description\"] ))\n",
    "              \n",
    "samples, _ = dataset_tools.preprocess(fileset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c312a216-220d-4cc6-bc53-86ceafdeffb5",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "The code in the cell below defines the processing of a single input file, which is processed in parallel with others. As was mentioned above, the data processed in this particular example is Montecarlo-generated. Here we process files generated for several physical processes. The number of events generated for the specific physical process is arbitrary, to correctly show the different processes on the same histogram one needs to normalise data to the same luminosity, which is in our case `LUMINOSITY = 36100`. It is worth mentioning that the events in this Monte Carlo data are weighted. \n",
    "\n",
    "To make normalization of the data with weighted events to respond to the given luminosity the following formula was used.\n",
    "\n",
    "`lumi_weight = LUMINOSITY * cross-section * k-factor * filter-efficiency / sum-of-mc-weights`\n",
    "\n",
    "Here `cross-section`, `k-factor` and `filter-efficiency` are defined in metadata. The `sum-of-mc-weights` is calculated from the stored in the PHYSLITE file as the sum of Montecarlo-weights for each physical process.\n",
    "\n",
    "The weight of each event during the filling histogram is `weight=lumi_weight*mc_weight`, where `lumi_weight` is constant and `mc_weight` is event specific (it is stored in the PHYSLITE file). \n",
    "\n",
    "In this code, the Monte Carlo-generated data for a single physical process can be split between multiple PHYSLITE files, each processed in parallel. For normalisation of the data one needs to divide weights on the `sum-of-mc-weights` across all events for a given physical process, but as the files are processed in parallel it is much more efficient to return the `sum-of-mc-weights` from the function which is doing the processing of single PHYSLITE file and divide each bin of the histogram on the `sum-of-mc-weights` outside of the parallel code. This approach is applied in the code below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331905ea-bece-421d-81b2-047ab413c2f5",
   "metadata": {},
   "source": [
    "TODO: Add the reference for the numerical value of luminosity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443b01ad-f3ec-4820-ada3-fe3d3976862b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "LUMINOSITY = 36100 \n",
    "\n",
    "def create_histogram(events):\n",
    "\n",
    "    reconstructed_top_mass, event_filters = calculate_trijet_mass_and_ev_filter(events)\n",
    "        \n",
    "    cross_section = events.metadata[\"crossSection_pb\"]\n",
    "    filter_efficiency = events.metadata[\"genFiltEff\"]\n",
    "    k_factor = events.metadata[\"kFactor\"]\n",
    "    process_label = events.metadata[\"process-label\"]\n",
    "    \n",
    "    sum_of_mc_weights = ak.sum(events.EventInfo.mcEventWeights[:, 0])\n",
    "    \n",
    "    mc_weight = events.EventInfo.mcEventWeights[:, 0][event_filters]\n",
    "    \n",
    "    lumi_weight = LUMINOSITY * cross_section * k_factor * filter_efficiency\n",
    "    \n",
    "    hist_reco_mtop = (hist.dask.Hist.new.Reg(25, 50, 550, name=\"m_reco_top\", label=r\"$m_{bjj}$ [GeV]\")\n",
    "                     .StrCat([], name=\"process\", label=\"Process\", growth=True)\n",
    "                     .Weight()\n",
    "                     .fill(reconstructed_top_mass/1000, weight=lumi_weight*mc_weight, process=process_label) )\n",
    "    \n",
    "    return {\"hist\":hist_reco_mtop, \"sum_of_file_weights\": sum_of_mc_weights}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff439aa-1997-4cd6-82f7-3cbaef759f17",
   "metadata": {},
   "source": [
    "The cell below attaches the files we prepared for parallel analysis to the dedicated function, `create_histogram` in our case. There we also specify branch filters and usage of `PHYSLITESchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd1bee4-76ce-49b5-8c5a-45da677dc6b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tasks = dataset_tools.apply_to_fileset(create_histogram, \n",
    "                                       samples, \n",
    "                                       schemaclass=PHYSLITESchema,\n",
    "                                       uproot_options={\"allow_read_errors_with_report\": (OSError, TypeError, KeyError), \"filter_name\": filter_name},\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2227c3e-04aa-4183-875b-9f199b655e28",
   "metadata": {},
   "source": [
    "The graph shows tasks for the histogram of the trijet mass of one of the physical processes considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53764596-0a03-49bc-a1b8-23b8ad88dfa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change the optimize_graph option to False to see a detailed graph.\n",
    "tasks[0][\"singletop_schan_lept_top\"][\"hist\"].visualize(optimize_graph=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118f2e17-8014-41fd-b96a-88883d9f91b7",
   "metadata": {},
   "source": [
    "The code below runs computations for parallel processing of multiple files which were delayed. Here we divide the counts in histogram bit by the corresponding `sum-of-mc-weights` as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc65ff-dc74-4a67-90c9-7c390e256d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with performance_report(filename=MEASUREMENT_PATH/\"dask-report-compute.html\"):\n",
    "    # execute\n",
    "    ((out, report), ) = dask.compute(tasks)\n",
    "    \n",
    "    # Dividing the number of counts in each bin in the histogram by the sum of weights.\n",
    "    for channel in out.keys():\n",
    "        out[channel][\"hist\"] /= out[channel][\"sum_of_file_weights\"]\n",
    "        \n",
    "        # Refactoring the structure because it is not wise to have a dictionary with only one element.\n",
    "        # And one does not need to have a sum of weights anymore.\n",
    "        out[channel] = out[channel][\"hist\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f950b05f-9f4a-4b73-922f-39476e372989",
   "metadata": {},
   "source": [
    "Let's print the reporting information of results of parallel computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c2210-0f80-4c6c-ba45-aedd30a078bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_uproot = ak.sum([v['duration'] for v in report.values()])\n",
    "print(f\"total time spent in uproot reading data: {time_uproot:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd3dfa3-1283-4858-a6de-68ba86538ff3",
   "metadata": {},
   "source": [
    "And finally making the histogram from the distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e52294-6589-4936-bcdf-dbddced6725b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_histogram_hist_mtop = sum([v for v in out.values()])\n",
    "\n",
    "\n",
    "# Draw the filled stacked histograms\n",
    "artists = full_histogram_hist_mtop.stack(\"process\").plot(\n",
    "    stack=True, histtype=\"fill\", linewidth=1,edgecolor=\"grey\",\n",
    ")\n",
    "\n",
    "\n",
    "# Draw the same histogram in the \"step\" style to get error bars on the same plot.\n",
    "artists = full_histogram_hist_mtop.stack(\"process\").plot(\n",
    "    stack=True, histtype=\"step\", linewidth=1, label=\"\", color=\"black\", yerr=True\n",
    ")\n",
    "\n",
    "\n",
    "ax = artists[0].stairs.axes\n",
    "fig = ax.get_figure()\n",
    "\n",
    "ax.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd72460-fce6-4cc3-bf94-843f7114a0f0",
   "metadata": {},
   "source": [
    "The code in the next cell will save the hist build on the data from each channel into the file. Please note that there are different channels with the same process label, for example, channels `singletop_schan_lept_top` and `singletop_schan_lept_antitop` have the same process label, which is `single top s chan`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ec110-a505-4391-a303-1157c63948d1",
   "metadata": {},
   "source": [
    "TODO: Document the cell below with comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1678276-31f0-4b66-8e9f-359f128634a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HIST_FILE_NAME = \"histograms.root\"\n",
    "\n",
    "# Saving the histograms of each channel into the file.\n",
    "with uproot.recreate(HIST_FILE_NAME) as f:\n",
    "        \n",
    "    # Declare empty pseudo data histogram which will be incremented on each cycle of the further loop.\n",
    "    pseudodata_hist = 0\n",
    "        \n",
    "    histograms_collection = []\n",
    "        \n",
    "    for channel, histogram in out.items():\n",
    "        \n",
    "        # There is only one sample for each channel in our data.\n",
    "        sample = histogram.axes[1][0]\n",
    "        current_hist = histogram[:, sample]\n",
    "        # f[channel] = current_hist\n",
    "        \n",
    "        histograms_collection.append({\"sample\": sample, \"histogram\": current_hist})\n",
    "        \n",
    "        # Adding the current histogram to the pseudo data\n",
    "        pseudodata_hist+=current_hist\n",
    "        \n",
    "        samples_uique = {el[\"sample\"] for el in histograms_collection}\n",
    "        \n",
    "        for sample in samples_uique:\n",
    "            \n",
    "            merget_hist = 0\n",
    "            \n",
    "            for el in histograms_collection:\n",
    "                \n",
    "                if el[\"sample\"] == sample:\n",
    "                    merget_hist += el[\"histogram\"]\n",
    "        \n",
    "            f[sample] = merget_hist\n",
    "        \n",
    "        \n",
    "    pseudodata_hist.view().value += np.random.normal( scale = np.sqrt(pseudodata_hist.values()) )\n",
    "        \n",
    "    f[\"pseudodata\"] = pseudodata_hist\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28463a2f-87f0-484d-ba09-c6be0ffa15e3",
   "metadata": {},
   "source": [
    "TODO: Write a description of what cabinetry is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2babc372-9961-4ca8-849b-e98cdc10bfb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear up in case using this cell repetitively\n",
    "!rm -f histograms/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7cb5f1-adba-4eb1-9b53-3c9ce5a1c47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cabinetry_config = cabinetry.configuration.load(\"cabinetry_config.yml\") # The code crashes at this line\n",
    "\n",
    "cabinetry.templates.collect(cabinetry_config, method=\"uproot\")\n",
    "cabinetry.templates.postprocess(cabinetry_config)\n",
    "\n",
    "workspace_path = \"workspaces/example_workspace.json\"\n",
    "ws = cabinetry.workspace.build(cabinetry_config)\n",
    "cabinetry.workspace.save(ws, workspace_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01735690-7032-4fae-88f4-70b6b2bb933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = cabinetry.model_utils.model_and_data(ws)\n",
    "fit_results = cabinetry.fit.fit(model, data)\n",
    "\n",
    "cabinetry.visualize.pulls(\n",
    "    fit_results, exclude=\"ttbar_norm\", close_figure=True, save_figure=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5498d2-cf43-4997-aafb-b37edfe206ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "poi_index = model.config.poi_index\n",
    "print(f\"\\nfit result for ttbar_norm: {fit_results.bestfit[poi_index]:.3f} +/- {fit_results.uncertainty[poi_index]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caad54e9-f458-4ed7-84f9-611d74be409a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_prediction = cabinetry.model_utils.prediction(model)\n",
    "model_prediction_postfit = cabinetry.model_utils.prediction(model, fit_results=fit_results)\n",
    "figs = cabinetry.visualize.data_mc(model_prediction, data, config=cabinetry_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c6a98c-ccc8-45d0-a950-6b19f3713567",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(figs[0]['figure'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
